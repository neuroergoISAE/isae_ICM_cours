{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d969c11b-c8f3-435f-9afa-988726d97be6",
   "metadata": {},
   "source": [
    "<h1 class=\"text-center\">BCI - Decoding frequency-tagging: a SSVEP-based BCI</h1>\n",
    "<h2 class=\"text-center\">February, 2022</h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "The purpose of this tutorial is to implement a reactive BCI using SSVEP on a dataset collected in our laboratory. You will use MNE to load and pre-process the data and Sklearn+MNE for the classification part. \n",
    "</b></div>\n",
    "\n",
    "- In Section I, exploration data analysis, frequency analysis and epoching using MNE\n",
    "- In Section II, a first classifier is trained on SNR at stimulation frequencies\n",
    "- In Section III, another pipeline that uses Canonical Correlation Analysis with sinus templates to learn a spatial filter.\n",
    "- üìú The last section (IV) is the evaluation. In Section IV, a more advance pipeline based on Task Related Correlation Analysis that add individual templates from calibration data. You would have 2 weeks to implement a classification pipeline using this classifier.\n",
    "\n",
    "The code must be completed after each ‚ùì **Question** ‚ùì. A blank cell with \"HERE\" appears as a comment in the code. The parameters that do not change the course of the story are accompanied \"EDIT ME!\" as a comment: you can change them at the time or at the end of the section to see the changes involved.\n",
    "\n",
    "You can also find some üî¥ HINTS üî¥ with associated links to documentation and usefull functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b361815-305e-44a5-9122-d9a643cc5d59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install packages\n",
    "Execute following cell to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce1a6bd-4233-408a-80d5-ec1fb746dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mne in /usr/local/lib/python3.8/dist-packages (1.2.3)\n",
      "Collecting mne\n",
      "  Downloading mne-1.3.0-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.9.3)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.1.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.2)\n",
      "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (21.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (2.25.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->mne) (2.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->mne) (2.1.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (4.33.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mne) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.8)\n",
      "Installing collected packages: scipy, joblib, scikit-learn, mne\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openvino-dev 2021.4.1 requires numpy<1.20,>=1.16.6, but you have numpy 1.22.0 which is incompatible.\n",
      "openvino-dev 2021.4.1 requires scipy~=1.5.4, but you have scipy 1.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.2.0 mne-1.3.0 scikit-learn-1.2.0 scipy-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U mne scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287882dd-b98c-4d02-ab93-42ea3baf64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.signal import welch\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from TRCA import TRCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ab1e9-6330-47e9-9d0b-7674734d8bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# I - Dataset exploration and pre-processing\n",
    "In this session we will work with data acquired at ISAE-SUPAERO on Steady States Visually Evoked Potentials (SSVEP). EEG data was collected using 32 Ag/AgCl active electrodes. A 32-channel montage based on the international 10-20 system was used to record the EEG signals with a sampling rate of 500Hz. The EEG device used in this experiment was the Brain Products LiveAmp system. Brain data was recorded using the LabRecorder software and the experimental protocol was implemented using the Psychopy Python library. Events from the experimental paradigm were synchronized with the EEG signal recording using the Lab Streaming Layer (LSL) library.\n",
    "\n",
    "**The subjects were asked to look at four different stimuli with rectangular shapes**. These stimuli flickered at different frequencies. Because of this difference in frequency, each stimuli elicits a different response in the Primary Visual Cortex, that we can classify in order to know at which target the participants were looking at each trial.\n",
    "\n",
    "## We load the data and plot the sensor location\n",
    "The data is presented in [EEGLAB .set format](https://eeglab.org/tutorials/03_Dataset_management/datasets.html). MNE supports data-loading functions in most common file formats in their `mne.io` module, check [here](https://mne.tools/0.18/manual/io.html#id15) for a complete list and link to the corresponding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ffce3-e28a-417a-bffa-d185804a2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/path/to/the/data'\n",
    "data_file = 'P1_low_100.set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce5615-8f71-4d2a-9e2b-036f73bd8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = os.path.join(data_dir, data_file)\n",
    "raw_data = mne.io.read_raw_eeglab(data_path, preload=True, verbose=False)\n",
    "\n",
    "# Show info (dict containing relevant metadata)\n",
    "print(raw_data.info)\n",
    "\n",
    "# Display the montage (sensors on the scalp)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "raw_data.plot_sensors(ch_type='eeg',show_names=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e44d6-b01b-4a3f-ba12-c3ae39d38560",
   "metadata": {},
   "source": [
    "## Now let us explore the EEG data.\n",
    "As previously, the data array has a shape of (channels, time). We use the `get_data()` method to obtain the EEG array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c08b1-f8da-46ad-97d0-bf2001c1fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = raw_data.get_data()\n",
    "\n",
    "# Print the shape of the data\n",
    "print(data_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a334ef-4f1e-4831-971d-4582fae31bb3",
   "metadata": {},
   "source": [
    "As previously stated, this data was acquired using a sampling frequency of 500Hz. We can recover this parameter from the `info` structure. This will be useful as we progress, but for now let's find out how long our data is in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be0f79-0322-4699-b77b-888cca36bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = raw_data.info['sfreq']  # Sampling frequency\n",
    "seconds = data_array.shape[-1] // sfreq\n",
    "print(f'Data duration in seconds: {seconds} (around {seconds // 60} minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744a60f-a855-48f0-abb9-c13cb3a6ba88",
   "metadata": {},
   "source": [
    "Data can be conveniently plotted from the raw object directly, allowing us some handy operations like filtering the data before displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314cec2d-d878-4f89-be5d-7197a68daac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "scal = dict(eeg=1e-3)                      # EDIT ME!\n",
    "raw_data.plot(n_channels=32, scalings=scal,\n",
    "              start=15, duration=2,             # EDIT ME!\n",
    "              lowpass=40, highpass=2,          # EDIT ME!\n",
    "              show_scrollbars=False, show_scalebars=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb421e08-fcce-4cb5-970b-ccb3061133e1",
   "metadata": {},
   "source": [
    "### Let's explore some of the events\n",
    "In this case, we find the events on an annotation file. Several ways of storing events exist, please refer to [the MNE documentation](https://mne.tools/dev/auto_tutorials/raw/20_event_arrays.html) to learn more about how to interact with different types of events. Here, we use the `events_from_annotations()` function to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387906a-e916-48d9-96f1-102ded1d6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations are part of the raw object\n",
    "print(raw_data.annotations)\n",
    "print()\n",
    "\n",
    "# We load the events and the event_id\n",
    "events, event_id = mne.events_from_annotations(raw_data, verbose=False)\n",
    "\n",
    "# event_id is a dictionary that related each label to their event name\n",
    "print(event_id)\n",
    "print()\n",
    "\n",
    "# The events are a list where each element is a 3-element list. The first element is the onset of the event, and the last one is the label according to event_id\n",
    "print(events[:10])\n",
    "\n",
    "# Not it is also a good time to extract labels from the events\n",
    "# With this our labels go from 0 to n_class - 1\n",
    "labels = events[:, -1]\n",
    "labels -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6042d06-86db-4901-9a97-1a4e1eeddd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display EEG signal with some events\n",
    "scal = dict(eeg=1e-3)     # EDIT ME!\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "raw_data.plot(events=events, event_color='red', event_id=event_id,\n",
    "              scalings=scal, clipping=None, show_scrollbars=False, show_scalebars=False, \n",
    "              lowpass=40, highpass=2,          # EDIT ME!\n",
    "              start=22, duration=40,  # EDIT ME!\n",
    "              n_channels=32)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3b911-248e-48ff-b250-1d509e7f11ab",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "We do not see a lot going on, for that we will have to move to the **frequency domain**, where we will be able to see and capture differences in frequency. Before that, we will have to pre-process the data. Different analysis require different pre-processing pipelines, and this time we will:\n",
    "\n",
    "- Keep the relevant channels\n",
    "- Band-pass filter the data\n",
    "- Epoch the data\n",
    "\n",
    "The only step that is new to this analysis is to keep a selection of channels. We will see an example first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0330d8-95dc-43aa-90b8-6f02637c04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep a selection of channels, first select the channels we want to keep\n",
    "ch_to_keep = [\"Fp1\", \"Fp2\"]\n",
    "\n",
    "# Make a list of the channels to drop\n",
    "ch_to_drop = list(set(raw_data.ch_names) - set(ch_to_keep))\n",
    "print(ch_to_drop)  # All the channels except ch_to_keep\n",
    "\n",
    "# Drop the rest of the channels using the drop_channels() function\n",
    "raw_data = raw_data.drop_channels(ch_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d60e9e-0bbc-40d9-acea-647030cb7105",
   "metadata": {},
   "source": [
    "#### ‚ùì **Question** ‚ùì: Load, pre-process, and epoch the data of a different subject using the functions presented until now. \n",
    "\n",
    "- Keep all the channels on the occipital (O) and parieto-occipital (PO) area\n",
    "- Keep in mind the frequency of the stimuli for filtering (i.e., make sure to capture all the stimulation frequencies)\n",
    "- Epoch between 0 and 2s, with a baseline of (0.2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8a44d-6b06-4830-9fe1-e06584eb27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "question_file = 'P2_low_100.set'\n",
    "\n",
    "data_path = os.path.join(data_dir, question_file)\n",
    "raw_data = # HERE\n",
    "\n",
    "# Get events and event_id\n",
    "events, event_id = # HERE\n",
    "\n",
    "# Drop all channels except selection\n",
    "ch_to_keep = # HERE\n",
    "ch_to_drop = # HERE\n",
    "\n",
    "raw_data = # HERE\n",
    "\n",
    "# Filter the data (notch and band-pass)\n",
    "raw_data = # HERE (notch filter)\n",
    "raw_data = # HERE (band-pass filter)\n",
    "\n",
    "# Epoch the data\n",
    "epochs = # HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd7f3a-2c6b-4ce5-a069-11f7f8dc7bcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### üî¥ HINTS üî¥: Functions to use. Open if you need a reminder of the functions necessary for the avobe exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f275a63-f312-4fe0-9c46-681135d2d3f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "- load data: `mne.io.read_raw_eeglab`\n",
    "- find events: `mne.events_from_annotations`\n",
    "- drop channels: `raw_data.drop_channels`\n",
    "- filter: `raw_data.filter`\n",
    "- epoch: `mne.Epochs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b94f5-f305-4170-95f4-eef98914042e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Additional üî¥ HINTS üî¥: Check if you feel lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7c133-d691-4b08-924f-2ca99069c3f8",
   "metadata": {},
   "source": [
    "# II - Frequency analysis\n",
    "We will now explore our data in the frequency domain, and use this information to tell apart the different stimulation frequencies. For that we will calculate the power spectral density (PSD) by calculating the Fourier Transform (FT).\n",
    "\n",
    "The goal of this section is to explore the frequency domain to observe the SSVEP response, as it is the principle that will allow us to classify later.\n",
    "\n",
    "## II-1 Using MNE functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcac041-394f-4cd8-b0b8-b8e8c731c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary parameters for the FT\n",
    "tmin = 0.\n",
    "tmax = 2.\n",
    "fmin = 1.\n",
    "fmax = 90.\n",
    "sfreq = epochs.info['sfreq']\n",
    "print(event_id)\n",
    "# In MNE, you can selection epoch based on labels\n",
    "spectrum = epochs['12.000000'].compute_psd('welch',\n",
    "                              n_fft=int(sfreq * (tmax - tmin)),\n",
    "                              n_overlap=0, n_per_seg=None,\n",
    "                              tmin=tmin, tmax=tmax,\n",
    "                              fmin=fmin, fmax=fmax,\n",
    "                              window='boxcar',\n",
    "                              verbose=False)\n",
    "\n",
    "psds, freqs = spectrum.get_data(return_freqs=True)\n",
    "psds = 10*np.log10(psds) # convert to dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5ffe2-0aaf-48ae-8a86-353801627f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9f202-5319-4280-9889-db916db85d2d",
   "metadata": {},
   "source": [
    "üî¥ HINTS üî¥  \n",
    "\n",
    "Not that clear with all the electrodes at once but it seems that we do have a peak a 12Hz and then another one at ~24Hz so an harmonic.\n",
    "\n",
    "## II-2 Using Scipy and Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7beff2-ecf2-41d3-89bb-22a5730a5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract epochs corresponding to a label\n",
    "\n",
    "data = epochs['12.000000'].get_data()\n",
    "\n",
    "f, psd = welch(data12, sfreq, nperseg=sfreq)\n",
    "psd_trial = np.mean(psd, axis=0)\n",
    "\n",
    "\n",
    "ch_names = epochs.info['ch_names']\n",
    "fig, axes = plt.subplots(len(ch_names), figsize=(5, 3 * len(ch_names)))\n",
    "\n",
    "for i, ch_name in enumerate(ch_names):\n",
    "    #axes[i].stem(f, np.sqrt(psd_trial[i]), linefmt='b', markerfmt=\" \", basefmt=\"-b\")\n",
    "    axes[i].plot(f,psd_trial[i])#linefmt='b', markerfmt=\" \", basefmt=\"-b\")\n",
    "    axes[i].set_xlabel('Freq (Hz)')\n",
    "    axes[i].set_ylabel('$\\mu V^2/Hz$')\n",
    "    #axes[i].set_yscale('log')\n",
    "    axes[i].title.set_text(f'Electrode: {ch_names[i]}')\n",
    "    \n",
    "    axes[i].set_xticks(range(0, 40, 2))\n",
    "    axes[i].set_xlim(0, 40)\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479dcbb3-a40e-4a3f-90a8-f7693f2437dd",
   "metadata": {},
   "source": [
    "üî¥ HINTS üî¥  \n",
    "\n",
    "We do have a clear peak on Oz !\n",
    "\n",
    "#### ‚ùì **Question** ‚ùì Explore with other labels and accross electrodes. \n",
    "\n",
    "üî¥ HINTS üî¥  \n",
    "You can try to use a dB scale to counter-balance the $\\frac{1}{f}$ law in the brain (more endougenous activy at low frequencies than higher ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e243a7-fb9a-4a6c-9be5-a11690522fce",
   "metadata": {
    "tags": []
   },
   "source": [
    "- You can check the signature of any function (how to call it, arguments, documentation, etc.) adding a '?' after its name in a jupyter notebook cell, for example, try running `mne.events_from_annotations?` on a new cell\n",
    "- You can check the list of all EEG channels in `raw_data.info['ch_names']`\n",
    "- You can check the stimulation frequencies with the `event_id` dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b5686-2415-480a-8fff-46dac53e796d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# III - First classification using Canonical Correlation Analysis (CCA)\n",
    "After having explored the frequency domain, we will now exploit this information to try to classify the different trials using various methods.\n",
    "\n",
    "The first of them is Canonical Correlation Analysis (CCA). This method takes two random multivariate variables, $X$ and $Y$, and finds a transformation vector that makes the two of them be maximally correlated (this correlation is the so-called 'Canonical Correlation'). For more information, you can read the [CCA page on the Scikit-Learn documentation](https://scikit-learn.org/stable/modules/cross_decomposition.html#canonical-correlation-analysis).\n",
    "\n",
    "The goal of this section is to lean how we can use CCA for our classification, and create a pipeline to classify the data from one of our participants. We will continue from the `epochs` object we created at the end of section I.\n",
    "\n",
    "## What are we comparing our signal to?\n",
    "As previously discussed, the stimulation were presented at certain frequencies. We also know that the brain activity in occipital areas shows a peak of activity at the frequency of stimulation. Knowing that, we can create 'artificial' signals that are perfect sinusoids at our target frequencies, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a945a0-11a6-435b-bb29-8e5756c0556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak = 12  # Target frequency\n",
    "trial_len = 1  # Length of the wave in seconds\n",
    "\n",
    "# Time points for our wave \n",
    "t = np.arange(0, trial_len, 1 / sfreq)\n",
    "sin = np.sin(2 * np.pi * peak * t)\n",
    "\n",
    "plt.plot(t, sin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc8501-88f8-430b-a181-e16b40272a6b",
   "metadata": {},
   "source": [
    "And just like that, we created a perfect sinusoid at one of the stimulation frequencies! To make things better for CCA, each frequency will be compared with a pair of sine and cosine waves at their frequency, as well as pairs of waves corresponding to their harmonics (i.e. freq * N), like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50963470-dbb1-4136-aed4-919d4fbda29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_harmonics = 2\n",
    "\n",
    "pairs = 1 + n_harmonics  # Include the frequency itself\n",
    "harmonics = [i + 1 for i in range(pairs)]  # This gives us the list of numbers we need to multiply our taget freq by (including 1)\n",
    "n_waves = pairs * 2  \n",
    "\n",
    "all_waves = []\n",
    "for i in harmonics:\n",
    "    target_freq = i * peak\n",
    "\n",
    "    sin = np.sin(2 * np.pi * (target_freq) * t)\n",
    "    cos = np.cos(2 * np.pi * (target_freq) * t)\n",
    "\n",
    "    all_waves.append(sin)\n",
    "    all_waves.append(cos)\n",
    "\n",
    "template = np.vstack(all_waves)  # (waves, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd7063-efc9-4b91-a502-31ce49369531",
   "metadata": {},
   "source": [
    "We will call this the 'template' for that particular stimulation frequency. We need to create one for each frequency and then, we will compare each trial of real data to all the templates, and we will consider the one that shows the highest correlation the predicted class of the data. \n",
    "\n",
    "#### ‚ùì **Question** ‚ùì: Make the templates for all classes in a single array\n",
    "\n",
    "- Define the list of target frequencies\n",
    "- Define the number of harmonics and total number of waves for each template\n",
    "- Define the length of the waves, that needs to be equal to the length of the data epochs (in samples)\n",
    "- Make an empty 3D array of shape (n_class, n_waves, trial_len)\n",
    "- Iterate over peak frequencies and then over the harmonics to make the pairs of waves, stack all waves in one array and put them on the empty 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898e3a5-0dd4-44dd-b03a-37347e0df503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target frequency list\n",
    "event_id = epochs.event_id\n",
    "peaks = # HERE\n",
    "\n",
    "# Get number of harmonics and total waves\n",
    "n_harmonics = 2\n",
    "pairs = 1 + n_harmonics  # Including the frequency itself\n",
    "\n",
    "harmonics = # HERE\n",
    "n_waves = # HERE\n",
    "\n",
    "# Get lenght of the wave (length of trials)\n",
    "data = epochs.get_data()\n",
    "trial_len = # HERE\n",
    "\n",
    "# Make time points 't'\n",
    "t = # HERE\n",
    "\n",
    "# Create empty array\n",
    "n_class = # HERE\n",
    "ref_signals = # HERE\n",
    "\n",
    "# Iterate over peaks (the index will be needed to add waves to ref_signals at the end\n",
    "for class_idx, peak in enumerate(peaks):\n",
    "    all_waves = []\n",
    "    \n",
    "    # Iterate over harmonics\n",
    "    for i in harmonics:\n",
    "        target_freq = # HERE \n",
    "\n",
    "        sin = # HERE\n",
    "        cos = # HERE\n",
    "\n",
    "        # Append the waves you just created \n",
    "        all_waves.append(sin)\n",
    "        all_waves.append(cos)\n",
    "        \n",
    "    # Stack to get an array of shape (waves, samples)\n",
    "    y = # HERE\n",
    "    \n",
    "    # Add to the empty array\n",
    "    # HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291cfe0-695a-473b-ba30-421c9f4922c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### üî¥ HINTS üî¥: Check here if you feel lost with the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb84f1-dd27-45a6-8bb9-2bbda20d53bb",
   "metadata": {},
   "source": [
    "- In order to get the length of the trials, you can retrieve the data array with `epochs.get_data()`. The shape of this array is (n_trials, n_channels, n_samples)\n",
    "- You can create a 3D array using the function `np.zeros()`. If you want an array of shape (3, 2, 4), you can make it with `np.zeros((3, 2, 4))`\n",
    "- To assign a 2D array to the first dimension of a 3D array (i.e., set a (2, 4) `template` array) as the first element of your (3, 2, 4) `all_templates` array, you can do it with `all_templates[0, :, :] = template`\n",
    "- As a reminder, the stimulation frequencies are stores in the `event_id` dict, that can be retrieved from your `epochs` with `epochs.event_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb4cc1-a728-480c-a1ee-e1326b0b684c",
   "metadata": {},
   "source": [
    "## CCA classification\n",
    "Now we have our `ref_signals` dict. Inside of it are the perfect signals that we will use for all our frequency stimulations. Now the next question is: How do we exactly use these to classify our data? The process is as follows:\n",
    "\n",
    "- We extract one epoch of data and find its true label from the `labels` array\n",
    "- We iterate over our target frequencies and select the corresponding template\n",
    "  - For each template, we fit a CCA model using our epoch and the template as $X$ and $Y$ variables\n",
    "  - We transform them using the CCA model\n",
    "  - We calculate the correlation between the transformed arrays and store it as the correlation for the corresponding class\n",
    "- Finally, we take the argmax of the list with all the correlations as the predicted label for that trial\n",
    "\n",
    "We will now see an example with one trial of data before proceeding to the final exercise of this section when you will implement the classification for all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76399e24-ed6a-47dd-a874-244eb1fab76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First trial\n",
    "ex_trial = data[0, ...]\n",
    "ex_label = labels[0]\n",
    "\n",
    "# Create the CCA model\n",
    "cca = CCA(n_components=1, max_iter=1000)\n",
    "\n",
    "# Empty list to store the correlations\n",
    "corrs = []\n",
    "\n",
    "# Iterate over classes\n",
    "for class_idx in range(len(peaks)):\n",
    "    # Get the corresponding template\n",
    "    template = ref_signals[class_idx, :, :]\n",
    "    \n",
    "    # Fit CCA and transform\n",
    "    cca.fit(ex_trial.T, template.T)\n",
    "    x_scores, y_scores = cca.transform(ex_trial.T, template.T)\n",
    "    \n",
    "    # Get correlation\n",
    "    corr_score = np.corrcoef(x_scores, y_scores, rowvar=False)[0, 1]\n",
    "    corrs.append(corr_score)\n",
    "    \n",
    "pred = np.argmax(corrs)\n",
    "print(f'True label: {ex_label}')\n",
    "print(f'Predicted label: {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa98db9-979a-441b-a684-87d9d6cd82ca",
   "metadata": {},
   "source": [
    "Now that we know how the process works for a single trial, it is time to do the same with all of them and put our newly acquired classifier to the test!\n",
    "\n",
    "#### ‚ùì **Question** ‚ùì: Build a CCA classifier and report the classification accuracy on one of our participants. You can use the same `epochs` and `ref_signals` from the previous exercises, we will focus only in building a loop for the classification.\n",
    "\n",
    "- Create an empty list `pred` for the prediced classes for all the trials\n",
    "- Create the CCA model at the beginning (no need to create a new one for each trial)\n",
    "- Loop over the data trials. For each trial:\n",
    "  - Repeat the single trial classification as before\n",
    "  - Append the predicted label to your `pred` list\n",
    "- Calculate and print the final accuracy\n",
    "\n",
    "No hints for this one, I believe in you :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560bf03a-bbae-4455-b5c8-e6b657add40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for predictions\n",
    "y_pred = []\n",
    "\n",
    "# CCA model\n",
    "cca = CCA(n_components=1, max_iter=1000)\n",
    "\n",
    "# Loop over trials\n",
    "n_trials, _, _ = data.shape\n",
    "\n",
    "    # Iterate over classes\n",
    "\n",
    "        # Fit CCA and transform\n",
    "\n",
    "        # Get correlation\n",
    "    \n",
    "    # Append the label of the max correlation to the pred list\n",
    "\n",
    "# Get accuracy\n",
    "\n",
    "# Print accuracy\n",
    "print(f'Total accuracy score: {acc_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2356555-3bcb-4573-8874-10b9c9558d92",
   "metadata": {},
   "source": [
    "Well done! We have a good performance, accounting for the fact that we are using artificial data as templates. Research has proposed using CCA but creating templates from the data itself, more similar to a traditional machine learning approach (train -> create templates, test -> classification). Since demonstrating it would be too similar to what we just did, we are now moving to a classification approach specific to SSVEP data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2511c3-0a89-4712-af5a-4ab7a03f1624",
   "metadata": {},
   "source": [
    "# IV - SSVEP classification using TRCA\n",
    "Task-Related Component Analysis (TRCA) is an approach that share many similarities (conceptually, at least) with CCA. On this section, we will cover the process of TRCA and implement a classification pipeline that leverages it for our SSVEP data. We will not dive into the specifics of the approach, nor we will implement every step from scratch, so for those interested in the 'guts' of the model, refer to [the original TRCA paper](https://ieeexplore.ieee.org/abstract/document/7904641)\n",
    "\n",
    "## How does TRCA work?\n",
    "Simply put, TRCA creates a \"class template\" for each frequency by averaging data trials of the same class. Then, using the templates for all classes, TRCA computes linear filters that maximize the similarity between examples of the same class, while maximizing the differences between examples of different classes. This is similar to how CCA allowed us to transform our data and template so they had maximum correlation. From here, the classification process is similar to what we did with CCA:\n",
    "\n",
    "- A data trial is extracted\n",
    "- The process iterates over all classes\n",
    "  - For each class, the data and the corresponding template are multiplied by the TRCA-filters\n",
    "  - Then, the correlation between the two is calculated\n",
    "- Finally, the class that holds the maximum correlation is considered the predicted class\n",
    "\n",
    "Additionally, TRCA leverages a filtering approach called 'filterbank'. Templates, filters and data are divided into 'bands' at different frequencies. All the process described above is performed for all the specified frequency bands and the results are combined before the classification decision is taken.\n",
    "\n",
    "## Classification\n",
    "We provide you with a sklearn-compatible TRCA. Similarly to other classification algorithms that you have used, it uses a `fit()` method to calculate the templates and spatial filter and a `predict()` method that will give you the predicted labels for your test data.\n",
    "\n",
    "## TRCA specifics\n",
    "Before moving on to classification, we have to note some particularities for TRCA. First, the authors describe the need to omit the first 0.14 seconds (approximately) of data after each stimulus presentation. The reason is that this is the time that the information takes to reach the visual cortex. Also, we will perform the classification on the first second of data. With all this, we will take our data epochs and select a slice from 0.14 to 1.14 seconds. Finally, we will take this chance to specify the number of bands for filterbank, as well as the downsample parameter for TRCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2e011-2ed0-46a2-893c-b57d0a0bc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_delay = 0.14\n",
    "epoch_len = 1.\n",
    "n_fbands = 4\n",
    "downsample = 2  # The original study downsampled to 250Hz, so we will do the same\n",
    "\n",
    "# Get t_min and t_max in samples to slice the data\n",
    "t_min = int(init_delay * sfreq) + 1\n",
    "t_max = int(t_min + epoch_len * sfreq)\n",
    "\n",
    "# Slice the data\n",
    "data_slice = data[..., t_min:t_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f06fd-2d1d-4852-9ae3-7cd75ac2724f",
   "metadata": {},
   "source": [
    "#### ‚ùì **Question** ‚ùì: Classify the data using TRCA.\n",
    "\n",
    "Now that we have an intuition about how TRCA works, the only thing left for us to do is to build a classification pipeline. For that, you will have to:\n",
    "\n",
    "- Start from the same `epochs` object we have been using so far\n",
    "- Split the data into train and test using an aprox of 33% testing data\n",
    "- Fit the train data and test on the testing data\n",
    "- Compute and print the accuracy of your classification\n",
    "\n",
    "When creating the TRCA model, you will have to provide the following arguments: `sfreq`, `n_fbands`, `peaks` and `downsample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8945c-39f4-4d83-8b58-0c1ce0a7a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = # HERE\n",
    "\n",
    "# Classifier\n",
    "clf = # HERE\n",
    "\n",
    "# Fit and predict\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = # HERE\n",
    "print(f'TRCA accuracy: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
